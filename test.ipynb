{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the coverage rate for all 5-node & 6-node patterns for all possible hyper-tree decomposition\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.algorithms.approximation import treewidth\n",
    "from networkx.algorithms.approximation.treewidth import treewidth_min_degree\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from pynauty import Graph, autgrp\n",
    "\n",
    "def nx_to_nauty(G):\n",
    "    ngraph = Graph(number_of_vertices=len(G.nodes()))\n",
    "    for u, v in G.edges():\n",
    "        ngraph.connect_vertex(u, v)\n",
    "    return ngraph\n",
    "\n",
    "def is_valid_group(G, group):\n",
    "    # A group is valid if no two nodes in the group are connected\n",
    "    for node1, node2 in combinations(group, 2):\n",
    "        if G.has_edge(node1, node2):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_all_partitions(G, nodes, current_partition, all_partitions):\n",
    "    if not nodes:\n",
    "        all_partitions.append(current_partition)\n",
    "        return\n",
    "    \n",
    "    first_node = nodes[0]\n",
    "    remaining_nodes = nodes[1:]\n",
    "\n",
    "    for i in range(len(current_partition)):\n",
    "        new_group = current_partition[i] | {first_node}\n",
    "        if is_valid_group(G, new_group):\n",
    "            find_all_partitions(G, remaining_nodes, current_partition[:i] + [new_group] + current_partition[i+1:], all_partitions)\n",
    "\n",
    "    # Start a new group with the first node\n",
    "    find_all_partitions(G, remaining_nodes, current_partition + [{first_node}], all_partitions)\n",
    "\n",
    "def get_all_partitions(G):\n",
    "    nodes = list(G.nodes())\n",
    "    all_partitions = []\n",
    "    find_all_partitions(G, nodes, [], all_partitions)\n",
    "    return all_partitions\n",
    "\n",
    "def is_covered(decomposition, partition):\n",
    "    for bag in decomposition:\n",
    "        if len(bag) <= 1:\n",
    "            continue\n",
    "        for group in partition:\n",
    "            if len(group) <= 1:\n",
    "                continue\n",
    "            if len(group.intersection(bag)) >= 2:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def load_pattern(pattern_dir):\n",
    "    patterns = {} # {'file_name': pattern}\n",
    "    for file in os.listdir(pattern_dir):\n",
    "        path = os.path.join(pattern_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            pattern = nx.Graph()\n",
    "            num_v, _ = map(int, f.readline().strip().split())\n",
    "            pattern.add_nodes_from(range(0, num_v))\n",
    "            for line in f.readlines()[:-1]:\n",
    "                u, v = line.strip().split()\n",
    "                pattern.add_edge(int(u), int(v))\n",
    "        patterns[os.path.splitext(file)[0]] = pattern\n",
    "    return patterns\n",
    "\n",
    "def compute_coverage_rate(pattern, verbose=False):\n",
    "    # print the pattern graph\n",
    "    if verbose:\n",
    "        nx.draw(pattern, with_labels=True)\n",
    "        plt.show()\n",
    "\n",
    "    # compute tree decomposition with the tree width\n",
    "    tw, decomposition = treewidth_min_degree(pattern)\n",
    "    if verbose:\n",
    "        print(\"True Treewidth:\", tw)\n",
    "        print(\"True Tree decomposition:\")\n",
    "        for i, bag in enumerate(decomposition.nodes):\n",
    "            print(f\"Bag {i}: {bag}\")\n",
    "        print()\n",
    "\n",
    "    # compute possible homorphism sub-pattern of the pattern\n",
    "    partitions = get_all_partitions(pattern)\n",
    "\n",
    "    # print all sub-patterns\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if verbose:\n",
    "            print(f\"Partition {i+1}: {partition}\")\n",
    "\n",
    "            # draw this patition\n",
    "            partition_graph = nx.Graph()\n",
    "            partition_graph.add_nodes_from(range(0, len(partition)))\n",
    "            for index_1, group_1 in enumerate(partition):\n",
    "                for index_2, group_2 in enumerate(partition):\n",
    "                    if group_1 == group_2:\n",
    "                        continue\n",
    "                    find_edge = False\n",
    "                    for node_1 in group_1:\n",
    "                        for node_2 in group_2:\n",
    "                            if pattern.has_edge(node_1, node_2):\n",
    "                                partition_graph.add_edge(index_1, index_2)\n",
    "                                find_edge = True\n",
    "                                break\n",
    "                        if find_edge:\n",
    "                            break\n",
    "            nx.draw(partition_graph, with_labels=True)\n",
    "            plt.show()\n",
    "    # compute the number of partitions that can be covered\n",
    "    coverage = 0\n",
    "    for partition in partitions:\n",
    "        if is_covered(decomposition, partition):\n",
    "            coverage += 1\n",
    "    if verbose:\n",
    "        print(f\"Coverage rate: {coverage}/{len(partitions)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        generators, grpsize1, grpsize2, orbits, numorbits = autgrp(nx_to_nauty(pattern))\n",
    "        print(pattern.nodes())\n",
    "        for auto in generators:\n",
    "            print(auto)\n",
    "        print(grpsize1, grpsize2, orbits, numorbits)\n",
    "\n",
    "    return coverage, len(partitions), tw\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # create the pattern\n",
    "    # pattern = nx.Graph()\n",
    "    # pattern.add_nodes_from([1, 2, 3, 4, 5, 6])\n",
    "    # pattern.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (3, 4), (4, 5), (2, 6), (3, 6), (4, 6), (5, 6)])\n",
    "\n",
    "    # load all 5voc, 6voc\n",
    "    voc5_dir = \"exp/pattern_graph/5voc\"\n",
    "    voc6_dir = \"exp/pattern_graph/6voc\"\n",
    "    voc5_patterns = load_pattern(voc5_dir)\n",
    "    voc6_patterns = load_pattern(voc6_dir)\n",
    "\n",
    "    # compute the coverage rate for all patterns\n",
    "    info = []\n",
    "    for name, pattern in voc5_patterns.items():\n",
    "        coverage, total, tw = compute_coverage_rate(pattern, verbose=True if name =='30' else False)\n",
    "        info.append((name, coverage, total, tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the file of the pattern in the paper\n",
    "import networkx as nx\n",
    "\n",
    "def load_pattern(pattern_dir):\n",
    "    patterns = {} # {'file_name': pattern}\n",
    "    for file in os.listdir(pattern_dir):\n",
    "        path = os.path.join(pattern_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            pattern = nx.Graph()\n",
    "            num_v, _ = map(int, f.readline().strip().split())\n",
    "            pattern.add_nodes_from(range(0, num_v))\n",
    "            for line in f.readlines()[:-1]:\n",
    "                u, v = line.strip().split()\n",
    "                pattern.add_edge(int(u), int(v))\n",
    "        patterns[os.path.splitext(file)[0]] = pattern\n",
    "    return patterns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create the pattern\n",
    "    # pattern = nx.Graph()\n",
    "    # pattern.add_nodes_from([1, 2, 3, 4, 5, 6])\n",
    "    # pattern.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (3, 4), (4, 5), (2, 6), (3, 6), (4, 6), (5, 6)])\n",
    "\n",
    "    pattern = nx.Graph()\n",
    "    pattern.add_nodes_from([0, 1, 2, 3, 4])\n",
    "    pattern.add_edges_from([(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (3, 4)])\n",
    "\n",
    "    voc5_patterns = load_pattern(voc5_dir)\n",
    "\n",
    "    # traverse all patterns and find the iso-match pattern in the paper\n",
    "    for name, p in voc5_patterns.items():\n",
    "        if nx.is_isomorphic(pattern, p):\n",
    "            print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# program to compute the left cosets\n",
    "from itertools import permutations\n",
    "\n",
    "def parse_permutation(perm_str):\n",
    "    \"\"\"Parses a permutation string like '(1)(2,3,4)(5)' into a permutation dict.\"\"\"\n",
    "    perm = {}\n",
    "    cycles = perm_str.split(')(')\n",
    "    for cycle in cycles:\n",
    "        cycle = cycle.replace('(', '').replace(')', '')\n",
    "        elements = list(map(int, cycle.split(',')))\n",
    "        for i in range(len(elements)):\n",
    "            perm[elements[i]] = elements[(i + 1) % len(elements)]\n",
    "    return perm\n",
    "\n",
    "def apply_permutation(perm, element):\n",
    "    \"\"\"Applies a permutation to a single element.\"\"\"\n",
    "    return perm.get(element, element)\n",
    "\n",
    "def multiply_permutations(perm1, perm2):\n",
    "    \"\"\"Multiplies two permutations (perm2 followed by perm1).\"\"\"\n",
    "    result = {}\n",
    "    all_keys = set(perm1.keys()).union(set(perm2.keys()))\n",
    "    for key in all_keys:\n",
    "        result[key] = apply_permutation(perm1, apply_permutation(perm2, key))\n",
    "    return result\n",
    "\n",
    "def format_permutation(perm):\n",
    "    \"\"\"Formats a permutation dict back into cycle notation.\"\"\"\n",
    "    seen = set()\n",
    "    cycles = []\n",
    "    for key in perm.keys():\n",
    "        if key not in seen:\n",
    "            current = key\n",
    "            cycle = []\n",
    "            while current not in seen:\n",
    "                seen.add(current)\n",
    "                cycle.append(current)\n",
    "                current = perm[current]\n",
    "            if len(cycle) >= 1:\n",
    "                cycles.append('(' + ','.join(map(str, cycle)) + ')')\n",
    "    return ''.join(cycles)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    group = ['(1)(2)(3)(4)(5)', '(1)(2,3,4)(5)', '(1)(2,4,3)(5)', '(1)(2,3)(4)(5)', '(1)(2,4)(3)(5)', '(1)(2)(3,4)(5)',\n",
    "             '(1,5)(2)(3)(4)', '(1,5)(2,3,4)', '(1,5)(2,4,3)', '(1,5)(2,3)(4)', '(1,5)(2,4)(3)', '(1,5)(2)(3,4)']\n",
    "    stabilizers = ['(1)(2)(3)(4)(5)', '(1)(2)(3,4)(5)', '(1,5)(2)(3)(4)', '(1,5)(2)(3,4)']\n",
    "\n",
    "    group_parsed = [parse_permutation(perm) for perm in group]\n",
    "    stabilizers_parsed = [parse_permutation(perm) for perm in stabilizers]\n",
    "    result = []\n",
    "    for g in group_parsed:\n",
    "        cosets = []\n",
    "        for stabilizer in stabilizers_parsed:\n",
    "            cosets.append(format_permutation(multiply_permutations(g, stabilizer)))\n",
    "        cosets.sort()\n",
    "        if cosets not in result:\n",
    "            result.append(cosets)\n",
    "    for cosets in result:\n",
    "        # subsutitute the permutation with the index + 1 in group array\n",
    "        cosets = [group.index(perm) + 1 for perm in cosets]\n",
    "        cosets.sort()\n",
    "        print(cosets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 67\u001b[0m\n\u001b[1;32m     64\u001b[0m subgraph_matching_counts \u001b[38;5;241m=\u001b[39m subgraph_matching_pattern_lsc[pattern_name]\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# load the pattern graph\u001b[39;00m\n\u001b[0;32m---> 67\u001b[0m pattern, orbit \u001b[38;5;241m=\u001b[39m load_pattern(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(pattern_dir, pattern_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.txt\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     69\u001b[0m \u001b[38;5;66;03m# compute the autormorphisms and orbits of the iterated pattern\u001b[39;00m\n\u001b[1;32m     70\u001b[0m generators, grpsize1, grpsize2, orbits, numorbits  \u001b[38;5;241m=\u001b[39m nauty\u001b[38;5;241m.\u001b[39mautgrp(pattern)\n",
      "Cell \u001b[0;32mIn[9], line 12\u001b[0m, in \u001b[0;36mload_pattern\u001b[0;34m(pattern_path)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m line \u001b[38;5;129;01min\u001b[39;00m f\u001b[38;5;241m.\u001b[39mreadlines()[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]:\n\u001b[1;32m     11\u001b[0m         graph\u001b[38;5;241m.\u001b[39mconnect_vertex(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mmap\u001b[39m(\u001b[38;5;28mint\u001b[39m, line\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()))\n\u001b[0;32m---> 12\u001b[0m     orbit \u001b[38;5;241m=\u001b[39m f\u001b[38;5;241m.\u001b[39mreadline()\u001b[38;5;241m.\u001b[39mstrip()\u001b[38;5;241m.\u001b[39msplit()[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m graph, orbit\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "# verify if the outputs from SCOPE can match the results from the SubgraphMatching program\n",
    "import os\n",
    "import pynauty as nauty\n",
    "\n",
    "\n",
    "def load_pattern(pattern_path):\n",
    "    raise NotImplementedError(\"This function is not implemented yet\")\n",
    "    with open(pattern_path, \"r\") as f:\n",
    "        vertex_num = f.readline().strip().split()[0]\n",
    "        graph = nauty.Graph(number_of_vertices=int(vertex_num))\n",
    "        for line in f.readlines()[:-1]:\n",
    "            graph.connect_vertex(*map(int, line.strip().split()))\n",
    "        orbit = f.readline().strip().split()[-1]\n",
    "    return graph, orbit\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # name of the data graph\n",
    "    data_graph = \"web-spam\"\n",
    "    pattern_size = \"5\"\n",
    "\n",
    "    # SCOPE directory\n",
    "    scope_dir = \"/home/ubuntu/Documents/workspace/dataset/{}/{}voc/\".format(data_graph, pattern_size)\n",
    "\n",
    "    # Subgraph Matching directory\n",
    "    subgraph_matching_dir = \"/home/ubuntu/Documents/workspace/dataset/iso_outputs_SubgraphMatch/{}/{}voc/\".format(data_graph, pattern_size)\n",
    "\n",
    "    # pattern directory\n",
    "    pattern_dir = \"/home/ubuntu/Documents/workspace/dataset/patterns/{}voc/\".format(pattern_size)\n",
    "\n",
    "    # data structures to store the results\n",
    "    scope_pattern_lsc = {} # {'pattern_name': [count, ...], ...}\n",
    "    subgraph_matching_pattern_lsc = {} # {'pattern_name': {data_vertex_id : count, ...}, ...}\n",
    "\n",
    "    # load the LSC results from the SCOPE directory\n",
    "    for file in os.listdir(scope_dir):\n",
    "        path = os.path.join(scope_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            countings = []\n",
    "            for line in f.readlines():\n",
    "                countings.append(int(line.strip()))\n",
    "            scope_pattern_lsc[os.path.splitext(file)[0]] = countings\n",
    "\n",
    "    # load the LSC results from the Subgraph Matching directory\n",
    "    for file in os.listdir(subgraph_matching_dir):\n",
    "        path = os.path.join(subgraph_matching_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            countings = []\n",
    "            for line in f.readlines():\n",
    "                if line.startswith(\"Vertex\"):\n",
    "                    countings.append(int(line.strip().split(\":\")[-1]))\n",
    "            subgraph_matching_pattern_lsc[os.path.splitext(file)[0]] = countings\n",
    "\n",
    "    if len(scope_pattern_lsc) != len(subgraph_matching_pattern_lsc):\n",
    "        print(\"Error: The number of patterns from SCOPE and Subgraph Matching are different\")\n",
    "        exit(1)\n",
    "\n",
    "    # obtain the automorphisms and orbits of the all patterns\n",
    "    automorphisms = {} # {'pattern_name': automorphisms_num, ...}\n",
    "    orbits = {} # {'pattern_name': {vertex_id, orbits_size}, ...}\n",
    "\n",
    "    # iterate the LSC results for each pattern\n",
    "    for pattern_name in scope_pattern_lsc.keys():\n",
    "        scope_counts = scope_pattern_lsc[pattern_name]\n",
    "        subgraph_matching_counts = subgraph_matching_pattern_lsc[pattern_name]\n",
    "\n",
    "        # load the pattern graph\n",
    "        pattern, orbit = load_pattern(os.path.join(pattern_dir, pattern_name + \".txt\"))\n",
    "\n",
    "        # compute the autormorphisms and orbits of the iterated pattern\n",
    "        generators, grpsize1, grpsize2, orbits, numorbits  = nauty.autgrp(pattern)\n",
    "\n",
    "        # Output the number of automorphisms and the generators of the automorphism group\n",
    "        automorphisms_num = int(grpsize1 * (10**grpsize2))\n",
    "\n",
    "        # modify Subgraph Matching's results according to the equation in SCOPE's paper\n",
    "\n",
    "        # compare the results from SCOPE and Subgraph Matching for the iterated pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
