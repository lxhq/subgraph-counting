{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print out the coverage rate for all 5-node & 6-node patterns for all possible hyper-tree decomposition\n",
    "\n",
    "import os\n",
    "import networkx as nx\n",
    "from networkx.algorithms.approximation import treewidth\n",
    "from networkx.algorithms.approximation.treewidth import treewidth_min_degree\n",
    "from itertools import combinations\n",
    "import matplotlib.pyplot as plt\n",
    "from pynauty import Graph, autgrp\n",
    "\n",
    "def nx_to_nauty(G):\n",
    "    ngraph = Graph(number_of_vertices=len(G.nodes()))\n",
    "    for u, v in G.edges():\n",
    "        ngraph.connect_vertex(u, v)\n",
    "    return ngraph\n",
    "\n",
    "def is_valid_group(G, group):\n",
    "    # A group is valid if no two nodes in the group are connected\n",
    "    for node1, node2 in combinations(group, 2):\n",
    "        if G.has_edge(node1, node2):\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "def find_all_partitions(G, nodes, current_partition, all_partitions):\n",
    "    if not nodes:\n",
    "        all_partitions.append(current_partition)\n",
    "        return\n",
    "    \n",
    "    first_node = nodes[0]\n",
    "    remaining_nodes = nodes[1:]\n",
    "\n",
    "    for i in range(len(current_partition)):\n",
    "        new_group = current_partition[i] | {first_node}\n",
    "        if is_valid_group(G, new_group):\n",
    "            find_all_partitions(G, remaining_nodes, current_partition[:i] + [new_group] + current_partition[i+1:], all_partitions)\n",
    "\n",
    "    # Start a new group with the first node\n",
    "    find_all_partitions(G, remaining_nodes, current_partition + [{first_node}], all_partitions)\n",
    "\n",
    "def get_all_partitions(G):\n",
    "    nodes = list(G.nodes())\n",
    "    all_partitions = []\n",
    "    find_all_partitions(G, nodes, [], all_partitions)\n",
    "    return all_partitions\n",
    "\n",
    "def is_covered(decomposition, partition):\n",
    "    for bag in decomposition:\n",
    "        if len(bag) <= 1:\n",
    "            continue\n",
    "        for group in partition:\n",
    "            if len(group) <= 1:\n",
    "                continue\n",
    "            if len(group.intersection(bag)) >= 2:\n",
    "                return False\n",
    "    return True\n",
    "\n",
    "def load_pattern(pattern_dir):\n",
    "    patterns = {} # {'file_name': pattern}\n",
    "    for file in os.listdir(pattern_dir):\n",
    "        path = os.path.join(pattern_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            pattern = nx.Graph()\n",
    "            num_v, _ = map(int, f.readline().strip().split())\n",
    "            pattern.add_nodes_from(range(0, num_v))\n",
    "            for line in f.readlines()[:-1]:\n",
    "                u, v = line.strip().split()\n",
    "                pattern.add_edge(int(u), int(v))\n",
    "        patterns[os.path.splitext(file)[0]] = pattern\n",
    "    return patterns\n",
    "\n",
    "def compute_coverage_rate(pattern, verbose=False):\n",
    "    # print the pattern graph\n",
    "    if verbose:\n",
    "        nx.draw(pattern, with_labels=True)\n",
    "        plt.show()\n",
    "\n",
    "    # compute tree decomposition with the tree width\n",
    "    tw, decomposition = treewidth_min_degree(pattern)\n",
    "    if verbose:\n",
    "        print(\"True Treewidth:\", tw)\n",
    "        print(\"True Tree decomposition:\")\n",
    "        for i, bag in enumerate(decomposition.nodes):\n",
    "            print(f\"Bag {i}: {bag}\")\n",
    "        print()\n",
    "\n",
    "    # compute possible homorphism sub-pattern of the pattern\n",
    "    partitions = get_all_partitions(pattern)\n",
    "\n",
    "    # print all sub-patterns\n",
    "    for i, partition in enumerate(partitions):\n",
    "        if verbose:\n",
    "            print(f\"Partition {i+1}: {partition}\")\n",
    "\n",
    "            # draw this patition\n",
    "            partition_graph = nx.Graph()\n",
    "            partition_graph.add_nodes_from(range(0, len(partition)))\n",
    "            for index_1, group_1 in enumerate(partition):\n",
    "                for index_2, group_2 in enumerate(partition):\n",
    "                    if group_1 == group_2:\n",
    "                        continue\n",
    "                    find_edge = False\n",
    "                    for node_1 in group_1:\n",
    "                        for node_2 in group_2:\n",
    "                            if pattern.has_edge(node_1, node_2):\n",
    "                                partition_graph.add_edge(index_1, index_2)\n",
    "                                find_edge = True\n",
    "                                break\n",
    "                        if find_edge:\n",
    "                            break\n",
    "            nx.draw(partition_graph, with_labels=True)\n",
    "            plt.show()\n",
    "    # compute the number of partitions that can be covered\n",
    "    coverage = 0\n",
    "    for partition in partitions:\n",
    "        if is_covered(decomposition, partition):\n",
    "            coverage += 1\n",
    "    if verbose:\n",
    "        print(f\"Coverage rate: {coverage}/{len(partitions)}\")\n",
    "    \n",
    "    if verbose:\n",
    "        generators, grpsize1, grpsize2, orbits, numorbits = autgrp(nx_to_nauty(pattern))\n",
    "        print(pattern.nodes())\n",
    "        for auto in generators:\n",
    "            print(auto)\n",
    "        print(grpsize1, grpsize2, orbits, numorbits)\n",
    "\n",
    "    return coverage, len(partitions), tw\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # create the pattern\n",
    "    # pattern = nx.Graph()\n",
    "    # pattern.add_nodes_from([1, 2, 3, 4, 5, 6])\n",
    "    # pattern.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (3, 4), (4, 5), (2, 6), (3, 6), (4, 6), (5, 6)])\n",
    "\n",
    "    # load all 5voc, 6voc\n",
    "    voc5_dir = \"exp/pattern_graph/5voc\"\n",
    "    voc6_dir = \"exp/pattern_graph/6voc\"\n",
    "    voc5_patterns = load_pattern(voc5_dir)\n",
    "    voc6_patterns = load_pattern(voc6_dir)\n",
    "\n",
    "    # compute the coverage rate for all patterns\n",
    "    info = []\n",
    "    for name, pattern in voc5_patterns.items():\n",
    "        coverage, total, tw = compute_coverage_rate(pattern, verbose=True if name =='30' else False)\n",
    "        info.append((name, coverage, total, tw))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the file of the pattern in the paper\n",
    "import networkx as nx\n",
    "import os\n",
    "\n",
    "def load_pattern(pattern_dir):\n",
    "    patterns = {} # {'file_name': pattern}\n",
    "    for file in os.listdir(pattern_dir):\n",
    "        path = os.path.join(pattern_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            pattern = nx.Graph()\n",
    "            num_v, _ = map(int, f.readline().strip().split())\n",
    "            pattern.add_nodes_from(range(0, num_v))\n",
    "            for line in f.readlines()[:-1]:\n",
    "                u, v = line.strip().split()\n",
    "                pattern.add_edge(int(u), int(v))\n",
    "        patterns[os.path.splitext(file)[0]] = pattern\n",
    "    return patterns\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # create the pattern\n",
    "    pattern = nx.Graph()\n",
    "    pattern.add_nodes_from([1, 2, 3, 4, 5, 6])\n",
    "    pattern.add_edges_from([(1, 2), (1, 3), (1, 4), (1, 5), (2, 3), (3, 4), (4, 5), (2, 6), (3, 6), (4, 6), (5, 6)])\n",
    "\n",
    "    # pattern = nx.Graph()\n",
    "    # pattern.add_nodes_from([0, 1, 2, 3, 4])\n",
    "    # pattern.add_edges_from([(0, 2), (0, 3), (0, 4), (1, 2), (1, 3), (1, 4), (2, 3), (3, 4)])\n",
    "\n",
    "    # pattern = nx.Graph()\n",
    "    # pattern.add_nodes_from([0, 1, 2, 3, 4])\n",
    "    # pattern.add_edges_from([(0, 1), (0, 2), (0, 3), (0, 4), (1, 3), (1, 2), (2, 3), (3, 4)])\n",
    "    \n",
    "    voc5_dir = \"exp/pattern_graph/5voc\"\n",
    "    voc6_dir = \"exp/pattern_graph/6voc\"\n",
    "    \n",
    "    patterns = load_pattern(voc6_dir)\n",
    "\n",
    "    # traverse all patterns and find the iso-match pattern in the paper\n",
    "    for name, p in patterns.items():\n",
    "        if nx.is_isomorphic(pattern, p):\n",
    "            print(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# program to compute the left cosets\n",
    "from itertools import permutations\n",
    "\n",
    "def parse_permutation(perm_str):\n",
    "    \"\"\"Parses a permutation string like '(1)(2,3,4)(5)' into a permutation dict.\"\"\"\n",
    "    perm = {}\n",
    "    cycles = perm_str.split(')(')\n",
    "    for cycle in cycles:\n",
    "        cycle = cycle.replace('(', '').replace(')', '')\n",
    "        elements = list(map(int, cycle.split(',')))\n",
    "        for i in range(len(elements)):\n",
    "            perm[elements[i]] = elements[(i + 1) % len(elements)]\n",
    "    return perm\n",
    "\n",
    "def apply_permutation(perm, element):\n",
    "    \"\"\"Applies a permutation to a single element.\"\"\"\n",
    "    return perm.get(element, element)\n",
    "\n",
    "def multiply_permutations(perm1, perm2):\n",
    "    \"\"\"Multiplies two permutations (perm2 followed by perm1).\"\"\"\n",
    "    result = {}\n",
    "    all_keys = set(perm1.keys()).union(set(perm2.keys()))\n",
    "    for key in all_keys:\n",
    "        result[key] = apply_permutation(perm1, apply_permutation(perm2, key))\n",
    "    return result\n",
    "\n",
    "def format_permutation(perm):\n",
    "    \"\"\"Formats a permutation dict back into cycle notation.\"\"\"\n",
    "    seen = set()\n",
    "    cycles = []\n",
    "    for key in perm.keys():\n",
    "        if key not in seen:\n",
    "            current = key\n",
    "            cycle = []\n",
    "            while current not in seen:\n",
    "                seen.add(current)\n",
    "                cycle.append(current)\n",
    "                current = perm[current]\n",
    "            if len(cycle) >= 1:\n",
    "                cycles.append('(' + ','.join(map(str, cycle)) + ')')\n",
    "    return ''.join(cycles)\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    group = ['(1)(2)(3)(4)(5)', '(1)(2,3,4)(5)', '(1)(2,4,3)(5)', '(1)(2,3)(4)(5)', '(1)(2,4)(3)(5)', '(1)(2)(3,4)(5)',\n",
    "             '(1,5)(2)(3)(4)', '(1,5)(2,3,4)', '(1,5)(2,4,3)', '(1,5)(2,3)(4)', '(1,5)(2,4)(3)', '(1,5)(2)(3,4)']\n",
    "    stabilizers = ['(1)(2)(3)(4)(5)', '(1)(2)(3,4)(5)', '(1,5)(2)(3)(4)', '(1,5)(2)(3,4)']\n",
    "\n",
    "    group_parsed = [parse_permutation(perm) for perm in group]\n",
    "    stabilizers_parsed = [parse_permutation(perm) for perm in stabilizers]\n",
    "    result = []\n",
    "    for g in group_parsed:\n",
    "        cosets = []\n",
    "        for stabilizer in stabilizers_parsed:\n",
    "            cosets.append(format_permutation(multiply_permutations(g, stabilizer)))\n",
    "        cosets.sort()\n",
    "        if cosets not in result:\n",
    "            result.append(cosets)\n",
    "    for cosets in result:\n",
    "        # subsutitute the permutation with the index + 1 in group array\n",
    "        cosets = [group.index(perm) + 1 for perm in cosets]\n",
    "        cosets.sort()\n",
    "        print(cosets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# verify if the outputs from SCOPE can match the results from the SubgraphMatching program\n",
    "import os\n",
    "import pynauty as nauty\n",
    "\n",
    "\n",
    "def load_pattern(pattern_path):\n",
    "    with open(pattern_path, \"r\") as f:\n",
    "        vertex_num, edge_num = map(int, f.readline().strip().split())\n",
    "        graph = nauty.Graph(number_of_vertices=int(vertex_num))\n",
    "        for _ in range(edge_num):\n",
    "            line = f.readline()\n",
    "            graph.connect_vertex(*map(int, line.strip().split()))\n",
    "        selected_vertex = int(f.readline().strip().split()[-1])\n",
    "    return graph, selected_vertex\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # name of the data graph\n",
    "    data_graph = \"web-spam\"\n",
    "    pattern_size = \"7\"\n",
    "    prefix_path = \"/home/ubuntu/Documents/workspace/dataset\"\n",
    "    # SCOPE directory\n",
    "    scope_dir = \"{}/{}/{}voc/\".format(prefix_path, data_graph, pattern_size)\n",
    "\n",
    "    # Subgraph Matching directory\n",
    "    subgraph_matching_dir = \"{}/iso_outputs_SubgraphMatch/{}/\".format(prefix_path, data_graph)\n",
    "\n",
    "    # pattern directory\n",
    "    pattern_dir = \"{}/patterns/{}voc/\".format(prefix_path, pattern_size)\n",
    "\n",
    "    # data structures to store the results\n",
    "    scope_pattern_lsc = {} # {'pattern_name': [count, ...], ...}\n",
    "    subgraph_matching_pattern_lsc = {} # {'pattern_name': {count, ...}, ...}\n",
    "\n",
    "    # load the LSC results from the SCOPE directory\n",
    "    for file in os.listdir(scope_dir):\n",
    "        path = os.path.join(scope_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            countings = []\n",
    "            for line in f.readlines():\n",
    "                countings.append(int(line.strip()))\n",
    "            scope_pattern_lsc[os.path.splitext(file)[0]] = countings\n",
    "\n",
    "    # load the LSC results from the Subgraph Matching directory\n",
    "    for file in os.listdir(subgraph_matching_dir):\n",
    "        if not file.startswith(str(pattern_size)):\n",
    "            continue\n",
    "        path = os.path.join(subgraph_matching_dir, file)\n",
    "        with open(path, \"r\") as f:\n",
    "            countings = []\n",
    "            for line in f.readlines():\n",
    "                if line.startswith(\"Vertex\"):\n",
    "                    countings.append(int(line.strip().split(\":\")[-1]))\n",
    "            subgraph_matching_pattern_lsc[os.path.splitext(file)[0].split('_')[-1]] = countings\n",
    "\n",
    "    if len(scope_pattern_lsc) != len(subgraph_matching_pattern_lsc):\n",
    "        print(\"Error: The number of patterns from SCOPE and Subgraph Matching are different\")\n",
    "        exit(1)\n",
    "\n",
    "    # obtain the automorphisms and orbits of the all patterns\n",
    "    automorphisms = {} # {'pattern_name': automorphisms_num, ...}\n",
    "    orbits = {} # {'pattern_name': {vertex_id, orbits_size}, ...}\n",
    "\n",
    "    # iterate the LSC results for each pattern\n",
    "    for pattern_name in scope_pattern_lsc.keys():\n",
    "        scope_counts = scope_pattern_lsc[pattern_name]\n",
    "        subgraph_matching_counts = subgraph_matching_pattern_lsc[pattern_name]\n",
    "\n",
    "        # load the pattern graph\n",
    "        pattern, selected_vertex = load_pattern(os.path.join(pattern_dir, pattern_name + \".txt\"))\n",
    "\n",
    "        # compute the autormorphisms and orbits of the iterated pattern\n",
    "        generators, grpsize1, grpsize2, orbits, numorbits  = nauty.autgrp(pattern)\n",
    "        automorphisms_grp_size = int(grpsize1 * (10**grpsize2))\n",
    "        selected_vertex_orbit_label = orbits[selected_vertex]\n",
    "        selected_vertex_orbits_grp_size = 0\n",
    "        for orbit in orbits:\n",
    "            if orbit == selected_vertex_orbit_label:\n",
    "                selected_vertex_orbits_grp_size += 1\n",
    "        \n",
    "        for scope_count, subgraph_matching_count in zip(scope_counts, subgraph_matching_counts):\n",
    "            # compare the results from SCOPE and Subgraph Matching for the iterated pattern\n",
    "            if scope_count != int(subgraph_matching_count * selected_vertex_orbits_grp_size / automorphisms_grp_size):\n",
    "                print(\"Error: The results of SCOPE and Subgraph Matching are different\")\n",
    "                exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert from a labeled graph to a unlabeled graph\n",
    "if __name__ == \"__main__\":\n",
    "    prefix = \"/home/ubuntu/Documents/workspace\"\n",
    "    data_graph = \"youtube\"\n",
    "    src_path = \"{}/dataset/{}/data_graph/{}.graph\".format(prefix, data_graph, data_graph)\n",
    "    dst_scope_path = \"{}/subgraph-counting/exp/data_graph/{}.txt\".format(prefix, data_graph)\n",
    "    dst_light_path = \"{}/LIGHT/sample_graph/{}.edge_list\".format(prefix, data_graph)\n",
    "\n",
    "    # read the edges from the src file\n",
    "    edges = []\n",
    "    num_vertices = 0\n",
    "    num_edges = 0\n",
    "    with open(src_path, \"r\") as f:\n",
    "        for line in f.readlines():\n",
    "            if line.startswith(\"e\"):\n",
    "                src, dest = map(int, line.strip().split()[1:3])\n",
    "                if src > num_vertices:\n",
    "                    num_vertices = src\n",
    "                if dest > num_vertices:\n",
    "                    num_vertices = dest\n",
    "                edges.append([src, dest])\n",
    "    num_edges = len(edges)\n",
    "    num_vertices += 1\n",
    "\n",
    "    # print the number of vertices and edges\n",
    "    print(num_vertices, num_edges)\n",
    "\n",
    "    # write the edges to the dst file\n",
    "    with open(dst_scope_path, \"w\") as f:\n",
    "        f.write(\"{} {}\\n\".format(num_vertices, num_edges))\n",
    "        for edge in edges:\n",
    "            f.write(\"{} {}\\n\".format(edge[0], edge[1]))\n",
    "    \n",
    "    # with open(dst_light_path, \"w\") as f:\n",
    "    #     for edge in edges:\n",
    "    #         f.write(\"{} {}\\n\".format(edge[0], edge[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert an unlabeled graph to a two-core only graph\n",
    "\n",
    "import networkx as nx\n",
    "\n",
    "# Function to extract the 2-core\n",
    "def extract_2_core(G):\n",
    "    return nx.k_core(G, k=2)\n",
    "\n",
    "# Function to write the new edge list to a file\n",
    "def write_edge_list(G, file_path):\n",
    "    with open(file_path, 'w') as f:\n",
    "        for edge in G.edges():\n",
    "            f.write(f\"{edge[0]} {edge[1]}\\n\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    prefix = \"/home/ubuntu/Documents/workspace/subgraph-counting/exp/data_graph\"\n",
    "    data_graph = \"youtube\"\n",
    "    src_path = \"{}/{}.txt\".format(prefix, data_graph)\n",
    "    dst_path = \"{}/{}_two_core.txt\".format(prefix, data_graph)\n",
    "\n",
    "    graph = nx.read_edgelist(src_path, nodetype=int)\n",
    "    graph_core_2 = nx.k_core(graph, k=2)\n",
    "\n",
    "    mapping = {old_label: new_label for new_label, old_label in enumerate(graph_core_2.nodes())}\n",
    "    G_continuous = nx.relabel_nodes(graph_core_2, mapping)\n",
    "    \n",
    "    with open(dst_path, 'w') as f:\n",
    "        f.write(f\"{G_continuous.number_of_nodes()} {G_continuous.number_of_edges()}\\n\")\n",
    "        for edge in G_continuous.edges():\n",
    "            f.write(f\"{edge[0]} {edge[1]}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
